{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c90da0",
   "metadata": {},
   "source": [
    "# PrescientPLMFold\n",
    "notebook adapted from [huggingface](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d828baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lobster.model import PrescientPLMFold\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb10c508",
   "metadata": {},
   "source": [
    "load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PrescientPLMFold(model_name=\"esmfold_v1\")  # pre-trained\n",
    "# m = PrescientPLMFold(model_name=\"PPLM\")  # random initialization for training from scratch\n",
    "m.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{sum(p.numel() for p in m.model.parameters() if p.requires_grad):,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.esm = m.model.esm.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.trunk.set_chunk_size(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a20357e",
   "metadata": {},
   "source": [
    "predict coordinates and convert to pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0629cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_protein = \"MGAGASAEEKHSRELEKKLKEDAEKDARTVKLLLLGAGESGKSTIVKQMKIIHQDGYSLEECLEFIAIIYGNTLQSILAIVRAMTTLNIQYGDSARQDDARKLMHMADTIEEGTMPKEMSDIIQRLWKDSGIQACFERASEYQLNDSAGYYLSDLERLVTPGYVPTEQDVLRSRVKTTGIIETQFSFKDLNFRMFDVGGQRSERKKWIHCFEGVTCIIFIAALSAYDMVLVEDDEVNRMHESLHLFNSICNHRYFATTSIVLFLNKKDVFFEKIKKAHLSICFPDYDGPNTYEDAGNYIKVQFLELNMRRDVKEIYSHMTCATDTQNVKFVFDAVTDIIIKENLKDCGLF\"\n",
    "\n",
    "tokenized_input = m.tokenizer([test_protein], return_tensors=\"pt\", add_special_tokens=False)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenized_input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = m.model(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf50aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26273ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hidden states from folding trunk\n",
    "output.states.shape  # (num structure blocks, B, L, sequence hidden dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.positions.shape  # (..., B, L, 14 (atom14), 3 (xyz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957edae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.plddt.shape  # (B, L, 37 (atom37))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4682a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_file = m.model.output_to_pdb(output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff3818",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.pdb', 'w') as f:\n",
    "    f.write(pdb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a097414",
   "metadata": {},
   "source": [
    "predict coordinates of a homodimer.\n",
    "\n",
    "we insert a \"linker\" of flexible glycine residues between each chain we want to fold simultaneously, and then we offset the position IDs for each chain from each other, so that the model treats them as being very distant portions of the same long chain  \n",
    "\n",
    "Tip: If you're trying to predict a multimeric structure and you're getting low-quality outputs, try varying the order of the chains (if it's a heteropolymer) or the length of the linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516da76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"MRLIPLHNVDQVAKWSARYIVDRINQFQPTEARPFVLGLPTGGTPLKTYEALIELYKAGEVSFKHVVTFNMDEYVGLPKEHPESYHSFMYKNFFDHVDIQEKNINILNGNTEDHDAECQRYEEKIKSYGKIHLFMGGVGVDGHIAFNEPASSLSSRTRIKTLTEDTLIANSRFFDNDVNKVPKYALTIGVGTLLDAEEVMILVTGYNKAQALQAAVEGSINHLWTVTALQMHRRAIIVCDEPATQELKVKTVKYFTELEASAIRSVK\"\n",
    "\n",
    "linker = 'G' * 25\n",
    "\n",
    "homodimer_sequence = sequence + linker + sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037da924",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_homodimer = m.tokenizer([homodimer_sequence], return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a large offset to the position IDs of the second chain\n",
    "with torch.no_grad():\n",
    "    position_ids = torch.arange(len(homodimer_sequence), dtype=torch.long)\n",
    "    position_ids[len(sequence) + len(linker):] += 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_homodimer['position_ids'] = position_ids.unsqueeze(0)\n",
    "\n",
    "tokenized_homodimer = {key: tensor.cuda() for key, tensor in tokenized_homodimer.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = m.model(**tokenized_homodimer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cafe947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the poly-G linker from the output, so we can display the structure as fully independent chains\n",
    "linker_mask = torch.tensor([1] * len(sequence) + [0] * len(linker) + [1] * len(sequence))[None, :, None]\n",
    "\n",
    "output['atom37_atom_exists'] = output['atom37_atom_exists'] * linker_mask.to(output['atom37_atom_exists'].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e363915",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_file = m.model.output_to_pdb(output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('homodimer.pdb', 'w') as f:\n",
    "    f.write(pdb_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10be21",
   "metadata": {},
   "source": [
    "## data stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c482291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lobster.data import FastaLightningDataModule\n",
    "from lobster.tokenization import PmlmTokenizer\n",
    "import importlib.resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = importlib.resources.files(\"lobster\") / \"assets\" / '3di_tokenizer'\n",
    "\n",
    "t = PmlmTokenizer.from_pretrained(path)\n",
    "inputs = ['GdPfQaPfIlSvRvLvEcQvClGpId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = t(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab293c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = t.decode(token_ids=tokenized_inputs['input_ids'][0], skip_special_tokens=True).replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d84201",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert inputs[0] == out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = FastaLightningDataModule(path_to_fasta=[\n",
    "    \"/scratch/site/u/freyn6/data/fasta/pdb_3di.fasta\",\n",
    "],\n",
    "                              tokenizer_dir='3di_tokenizer',\n",
    "                             batch_size=4,\n",
    "                             )\n",
    "dm.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eebbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm._transform_fn._auto_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da6463",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36825c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lobster",
   "language": "python",
   "name": "lobster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
