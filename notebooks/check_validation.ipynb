{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a4f40f-d6f9-48bc-a970-a7bc59120b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from try_load_dataset import find_latest_ckpt_file, load_datamodule_from_config, deep_compare\n",
    "from lobster.model import LobsterPCLM2, LobsterPCLM\n",
    "import sys\n",
    "\n",
    "# QM9 and RDKit imports\n",
    "from atomic_datasets import QM9\n",
    "from atomic_datasets.utils.rdkit import is_molecule_sane\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs, rdFingerprintGenerator\n",
    "import selfies as sf\n",
    "\n",
    "# Import our utility functions\n",
    "#from utils_qm9 import get_shape_tanimoto\n",
    "import pandas as pd\n",
    "#from qm9_pair_generation import is_valency_ok\n",
    "from qm9_pair_gen.utils_mol import *\n",
    "import sys\n",
    "sys.path.append('/homefs/home/lawrenh6/lobster/src/lobster/callbacks')\n",
    "from _molecule_validation_callback import MoleculeValidationCallback\n",
    "import torch\n",
    "\n",
    "# Define the test input molecule\n",
    "test_mol = \"CN(C)C[C@@H](O)[C@@H](c1ccccc1)c1ccc(Cl)cc1\"\n",
    "\n",
    "# Determine the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ef257d-90c6-4260-9766-23bdc4afec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_dir: ../src/lobster/hydra_config\n",
      "config_name: train_molecule_improvement\n",
      "original_cwd /homefs/home/lawrenh6/lobster/notebooks\n",
      "hydra initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:try_load_dataset:Configuration loaded:\n",
      "INFO:try_load_dataset:dryrun: false\n",
      "run_test: null\n",
      "compile: false\n",
      "seed: 42\n",
      "logger:\n",
      "  project: null\n",
      "  name: null\n",
      "  entity: null\n",
      "  _target_: lightning.pytorch.loggers.WandbLogger\n",
      "  save_dir: .\n",
      "  offline: false\n",
      "  group: null\n",
      "  notes: null\n",
      "  tags: null\n",
      "paths:\n",
      "  root_dir: first_run\n",
      "  output_dir: ${paths.root_dir}/${paths.timestamp}\n",
      "  timestamp: ${now:%Y-%m-%d}T${now:%H-%M-%S.%f}\n",
      "data:\n",
      "  _target_: lobster.data._molecule_improvement_datamodule.MoleculeImprovementLightningDataModule\n",
      "  root: /data/lawrenh6/cache/test_new_code/qm9_pairs_per_mol_5_full\n",
      "  train_pair_filename: pairs_train.parquet\n",
      "  val_pair_filename: pairs_val.parquet\n",
      "  test_pair_filename: pairs_test.parquet\n",
      "  utility_key: gap\n",
      "  shape_tanimoto_percentile: null\n",
      "  shape_tanimoto_num_pairs: 50\n",
      "  delta: None\n",
      "  epsilon: 0.001\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_workers: 4\n",
      "  pin_memory: true\n",
      "  drop_last: true\n",
      "  max_train_samples: null\n",
      "  transform_fn:\n",
      "    _target_: lobster.transforms.TokenizerTransform\n",
      "    tokenizer:\n",
      "      _target_: lobster.tokenization.SmilesTokenizer2Fast\n",
      "    padding: max_length\n",
      "    truncation: true\n",
      "    max_length: ${model.max_length}\n",
      "model:\n",
      "  _target_: lobster.model.LobsterPCLM2\n",
      "  model_name: CLM_150M\n",
      "  lr: 0.0001\n",
      "  max_length: 1024\n",
      "  ckpt_path: null\n",
      "  num_training_steps: ${trainer.max_steps}\n",
      "  num_warmup_steps: 1000\n",
      "  num_validation_generations: 40\n",
      "  tokenizer_dir: garbage_test\n",
      "  transform_fn: ${data.transform_fn}\n",
      "  model_kwargs:\n",
      "    embedding_layer: linear_pos\n",
      "    hidden_act: gelu\n",
      "  scheduler_kwargs: null\n",
      "callbacks:\n",
      "  model_checkpoint:\n",
      "    _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "    dirpath: ${paths.output_dir}\n",
      "    filename: '{epoch}-{step}-{val_loss:.4f}'\n",
      "    monitor: val_loss\n",
      "    save_top_k: 3\n",
      "    mode: min\n",
      "    verbose: true\n",
      "    save_last: true\n",
      "  lr_monitor:\n",
      "    _target_: lightning.pytorch.callbacks.LearningRateMonitor\n",
      "    logging_interval: step\n",
      "  progress_bar:\n",
      "    _target_: lightning.pytorch.callbacks.TQDMProgressBar\n",
      "    refresh_rate: 100\n",
      "  early_stopping:\n",
      "    monitor: val_loss\n",
      "  molecule_validation:\n",
      "    _target_: lobster.callbacks.MoleculeValidationCallback\n",
      "    num_validation_generations: ${model.num_validation_generations}\n",
      "trainer:\n",
      "  _target_: lightning.pytorch.Trainer\n",
      "  accelerator: gpu\n",
      "  devices: 1\n",
      "  num_nodes: 1\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 0.0\n",
      "  gradient_clip_algorithm: norm\n",
      "  precision: 32\n",
      "  max_steps: 100000000\n",
      "  val_check_interval: 0.25\n",
      "  limit_val_batches: 10\n",
      "  num_sanity_val_steps: 2\n",
      "  max_time: null\n",
      "setup:\n",
      "  torch:\n",
      "    _target_: torch.set_float32_matmul_precision\n",
      "    precision: medium\n",
      "  seed:\n",
      "    _target_: lightning.pytorch.seed_everything\n",
      "    seed: 15855310\n",
      "    workers: true\n",
      "lr_scheduler:\n",
      "  scheduler:\n",
      "    _target_: transformers.optimization.get_linear_schedule_with_warmup\n",
      "    num_warmup_steps: ${model.num_warmup_steps}\n",
      "    num_training_steps: ${model.num_training_steps}\n",
      "\n",
      "Seed set to 15855310\n",
      "INFO:try_load_dataset:Instantiating datamodule...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Setting up datamodule for stage: fit\n",
      "Loaded 5480540 pairs for split 'train' using utility 'gap'\n",
      "After shape Tanimoto num_pairs filtering (50 pairs per molecule): 5480540 pairs\n",
      "After utility filtering (> 0.001): 5204656 pairs\n",
      "Loaded 87450 pairs for split 'val' using utility 'gap'\n",
      "After shape Tanimoto num_pairs filtering (50 pairs per molecule): 87450 pairs\n",
      "After utility filtering (> 0.001): 897 pairs\n",
      "Loaded 64375 pairs for split 'test' using utility 'gap'\n",
      "After shape Tanimoto num_pairs filtering (50 pairs per molecule): 64375 pairs\n",
      "After utility filtering (> 0.001): 688 pairs\n"
     ]
    }
   ],
   "source": [
    "overrides = None\n",
    "config_path = '../src/lobster/hydra_config/train_molecule_improvement.yaml' #train_chembl.yaml'\n",
    "datamodule, cfg, transform_fn = load_datamodule_from_config(config_path, overrides=overrides)\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "datamodule.prepare_data()\n",
    "\n",
    "stage = \"fit\"\n",
    "print(f\"Setting up datamodule for stage: {stage}\")\n",
    "datamodule.setup(stage=stage)\n",
    "\n",
    "dsets = {'train': datamodule._train_dataset, 'val': datamodule._val_dataset, 'test': datamodule._test_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d82f8e1-3c42-4689-89c4-3e4a53425809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lawrenh6/lobster_runs/val_initialized_propen_4/2025-08-12T06-51-27.224517/epoch=0-step=20330-val_loss=0.5613.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the LobsterPMLM model\n",
    "\n",
    "ckpt_path = find_latest_ckpt_file('/data/lawrenh6/lobster_runs/val_initialized_propen_4', use_val_loss=True) # serious_chembl_large2\n",
    "print(ckpt_path)\n",
    "\n",
    "model = LobsterPCLM2(\"CLM_150M\", transform_fn=transform_fn).to(device) # CLM_mini # CLM_150M  # CLM_mini\n",
    "model.eval()\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "model.load_state_dict(ckpt['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f56821-7024-4f99-b089-4e7d181a63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72dd50fa-671a-47da-856b-f3ceec55d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 8, 16, 10, 8, 11, 8, 20, 10, 12, 11, 20, 10, 9, 13, 9, 9, 9, 9, 9, 13, 11, 9, 13, 9, 9, 9, 10, 26, 11, 9, 9, 13, 2]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(test_mol)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2a6a95c-da01-4856-a314-2d4928a8724d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 53,  8,  ...,  1,  1,  1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets['train'][10]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "321a57c9-57d2-4650-bfef-2eeceaccd277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[H]C1([H])C([H])([H])C([H])([H])C([H])([H])C([H])([H])C([H])([H])C1([H])[H]',\n",
       " '[H]C([H])([H])[H]')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MoleculeValidationCallback._tokens_to_smiles(tokenizer, dsets['train'][10]['input_ids'].reshape(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efac2409-6b1e-4ff9-a2f0-73ae935cd3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN(C)C[C@@H](O)[C@@H](c1ccccc1)c1ccc(Cl)cc1 None\n"
     ]
    }
   ],
   "source": [
    "a, b = MoleculeValidationCallback._tokens_to_smiles(tokenizer, tokenizer.encode(test_mol))\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a13897f4-3512-4b50-b239-a1ff7b821b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _molecule_validation_callback import short_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d8165a-eded-4016-ad12-3c883c52ecf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CN(C)C[C@@H](O)[C@@H](c1ccccc1)c1ccc(Cl)cc1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8150f4af-beae-48f9-8093-fa06277a8699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CN(C)C[C@@H](O)[C@@H](c1ccccc1)c1ccc(Cl)cc1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0c8662-5a4b-4d93-afa4-a320572a1600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LobsterPCLM2(\n",
       "  (_transform_fn): TokenizerTransform()\n",
       "  (model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(1226, 816, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0-19): 20 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=816, out_features=816, bias=False)\n",
       "            (k_proj): Linear(in_features=816, out_features=816, bias=False)\n",
       "            (v_proj): Linear(in_features=816, out_features=816, bias=False)\n",
       "            (o_proj): Linear(in_features=816, out_features=816, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=816, out_features=2048, bias=False)\n",
       "            (up_proj): Linear(in_features=816, out_features=2048, bias=False)\n",
       "            (down_proj): Linear(in_features=2048, out_features=816, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((816,), eps=1e-06)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((816,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((816,), eps=1e-06)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=816, out_features=1226, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LobsterPCLM2(\"CLM_150M\", transform_fn=transform_fn).to(device) # CLM_mini # CLM_150M  # CLM_mini\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36a88fe0-253f-47ac-94dd-998b49fdf34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer._tokenizer.post_processor = None  # changes by reference, seemingly -- so \n",
    "generated = model.sample(seed_seq=\"<cls>\", temperature=0.95, max_length=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44f9991f-429c-4227-aa45-65381cc42ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated=out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14ec86cf-bfb2-4658-88e0-8cb35e306357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls> <cls> <eos>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"<cls>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "104750b2-745b-49c4-9433-aa6dbf426629",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gen_ids = \u001b[43mgenerated\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m().cpu()\n\u001b[32m      2\u001b[39m gen_full_str = pl_module.tokenizer.decode(gen_ids.tolist())\n\u001b[32m      3\u001b[39m gen_new_ids = gen_ids[\u001b[38;5;28mlen\u001b[39m(unmasked_ids):]\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "gen_ids = generated[0].detach().cpu()\n",
    "gen_full_str = pl_module.tokenizer.decode(gen_ids.tolist())\n",
    "gen_new_ids = gen_ids[len(unmasked_ids):]\n",
    "gen_new_str = pl_module.tokenizer.decode(gen_new_ids.tolist()) if gen_new_ids.numel() > 0 else \"\"\n",
    "\n",
    "gen_ids = generated[0].detach().cpu()\n",
    "gen_full_str = pl_module.tokenizer.decode(gen_ids.tolist()) # space separated string\n",
    "gen_new_ids = gen_ids[len(unmasked_ids):]\n",
    "gen_new_str = pl_module.tokenizer.decode(gen_new_ids.tolist()) if gen_new_ids.numel() > 0 else \"\"\n",
    "\n",
    "\n",
    "MoleculeValidationCallback._tokens_to_smiles(model.tokenizer, generated_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c038ef4-1398-4f8a-a78e-e55f2a36248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = out[0].split(\" \")\n",
    "temp[0] = temp[0][5:]\n",
    "smiles = ''.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "802bab7c-1e02-4b71-ba56-e8d3e095fd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99Tc+3][89Zr][Lu][Al+3][H-][Ac-][18OH2][33PH][18OH2][Au+][Rh-3][PH-2][111InH3][13N+][12CH2][Sn+2]%23[AlH+2][Ir-][Tm+3][Cr+2][Se@][PoH2][Hs][89Zr+4][S-][54Cr][Pb+3]%48[PH3][7NaH]4[Hg][F-][Bi+5][Rn][Rh+4][Zr+2][Sb+][SrH2][Te+4][13NH2][12cH]%91[PbH2][Li-][177Lu][Ac-]([52Mn+2][Ca+2][Na-2][Pm][Mn+3][68Ga][20CH3][PbH2][ZrH3][Sc+2]5[Tl][BH2-][Al-]/[Xe+][18CH][15nH+][11cH][Pt-][NbH2]%28%48[Ti+5][137Cs+][U+3][Nb+3][SnH2+][18FH][V+2][Ge@@][B@@-]%78[LaH][Pm][Nb+2][SbH4][NH3+2][Ru-3][Dy+3][AlH+][14CH2-][B@-][11CH3][WH][Tc][Ir+4][cH+]%82[Ag-]\\[PbH4]$[OsH6][Sn+2][14CH2][Nd+][68GaH3][Ir+2][Ru-3][SH][Os+6][Ce+4]%14[Er][ClH2+][I-][Dy+3][PH-][13CH][Ru+8][188Re][Te+4][Al-][18O][Si@@][ArH][Tc+2][Sg]%24[209Po]%54[67Cu+2][208Po][SH2+][Mn+4][125I][Cu-2]%43[Fe+6][133I][Si@@H][68Zn][Rh-3][Pd-][PtH3][Sr][Li-][SbH2][BH-][CH+][CoH+2][GeH2][13CH-][Si@][121I][122Xe][Ho+3][111InH3][15O][V][Pd+2][10C][Ce][62Ni][Al][Pr+3]\\%60[PH-2][Pb+3][SbH-][Rh+][13C@@H][CH3][Al-][10CH2][Cu-][C+4][81Kr][123I-][36SH2][NH2][BH][SnH+3][Ir-][10B-][Sn+2][Zn-2][34SH2][13O][SH+][BiH5][CH3+]N[202Po][AsH2+][Rb+][LaH][Tc+2][Si+][SeH2+][Rh-3][U][Ho][57Fe]%90[IH2+][15NH2+][239Th][Ni-][99Tc+5][Nb+2][BH+][Fe+4][Ge@@]%63[U][XeH2][22CH3-][nH+][12CH][WH][Pt+][CeH3][216Bi][16n+][Mg+][82Rb+][PH2][SH5][AlH5-2][16OH][Nd][B@][198Au][14C][Re][SiH3+][47Ca+2][CH3+][PbH3][15NH2+][54Cr][Be+2]%21[Au+3][Nb+3][64Ni]%65[Sb][AlH+2][Fe+4][SeH][123I][Cu-2]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[99Tc+3][89Zr][Lu][Al+3][H-][Ac-][18OH2][33PH][18OH2][Au+][Rh-3][PH-2][111InH3][13N+][12CH2][Sn+2]%23[AlH+2][Ir-][Tm+3][Cr+2][Se@][PoH2][Hs][89Zr+4][S-][54Cr][Pb+3]%48[PH3][7NaH]4[Hg][F-][Bi+5][Rn][Rh...\n"
     ]
    }
   ],
   "source": [
    "print(smiles)\n",
    "print('\\n\\n\\n')\n",
    "print(short_print(smiles, n=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94b6c06e-2985-4f76-bc7a-33d45055b750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_print(smiles, n=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b4fa12-19e0-4202-8b38-568ffbae7e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envlob",
   "language": "python",
   "name": "envlob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
