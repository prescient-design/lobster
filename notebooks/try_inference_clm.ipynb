{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd6fdd50-a898-47ff-95e8-b778edb70c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# Lobster Model Inference Notebook\n",
    "import torch\n",
    "from lobster.model import LobsterPCLM2, LobsterPCLM\n",
    "import lobster\n",
    "from try_load_dataset import find_latest_ckpt_file, load_datamodule_from_config, deep_compare\n",
    "import os\n",
    "\n",
    "# Define the test input molecule\n",
    "test_mol = \"CN(C)C[C@@H](O)[C@@H](c1ccccc1)c1ccc(Cl)cc1\"\n",
    "\n",
    "# Determine the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from atomic_datasets.utils import is_molecule_sane\n",
    "#import qm9_pair_gen\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e19d5-cec3-474c-8214-51400ab69d03",
   "metadata": {},
   "source": [
    "# Load datamodule, which includes the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92a0cfe-ed44-4416-bb7a-d9392bb1d348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_dir: ../src/lobster/hydra_config\n",
      "config_name: train_molecule_improvement\n",
      "original_cwd /homefs/home/lawrenh6/lobster/notebooks\n",
      "hydra initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:try_load_dataset:Configuration loaded:\n",
      "INFO:try_load_dataset:dryrun: false\n",
      "run_test: null\n",
      "compile: false\n",
      "seed: 42\n",
      "logger:\n",
      "  project: null\n",
      "  name: null\n",
      "  entity: null\n",
      "  _target_: lightning.pytorch.loggers.WandbLogger\n",
      "  save_dir: .\n",
      "  offline: false\n",
      "  group: null\n",
      "  notes: null\n",
      "  tags: null\n",
      "paths:\n",
      "  root_dir: first_run\n",
      "  output_dir: ${paths.root_dir}/${paths.timestamp}\n",
      "  timestamp: ${now:%Y-%m-%d}T${now:%H-%M-%S.%f}\n",
      "data:\n",
      "  _target_: lobster.data._molecule_improvement_datamodule.MoleculeImprovementLightningDataModule\n",
      "  root: /data/lawrenh6/cache/test_new_code/qm9_pairs_per_mol_5_full\n",
      "  train_pair_filename: pairs_train.parquet\n",
      "  val_pair_filename: pairs_val.parquet\n",
      "  test_pair_filename: pairs_test.parquet\n",
      "  utility_key: gap\n",
      "  delta: None\n",
      "  epsilon: 0.001\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_workers: 4\n",
      "  pin_memory: true\n",
      "  drop_last: true\n",
      "  max_train_samples: null\n",
      "  transform_fn:\n",
      "    _target_: lobster.transforms.TokenizerTransform\n",
      "    tokenizer:\n",
      "      _target_: lobster.tokenization.SmilesTokenizer2Fast\n",
      "    padding: max_length\n",
      "    truncation: true\n",
      "    max_length: ${model.max_length}\n",
      "model:\n",
      "  _target_: lobster.model.LobsterPCLM2\n",
      "  model_name: CLM_150M\n",
      "  lr: 0.0001\n",
      "  max_length: 1024\n",
      "  ckpt_path: null\n",
      "  num_training_steps: ${trainer.max_steps}\n",
      "  num_warmup_steps: 1000\n",
      "  num_validation_generations: 40\n",
      "  tokenizer_dir: garbage_test\n",
      "  transform_fn: ${data.transform_fn}\n",
      "  model_kwargs:\n",
      "    embedding_layer: linear_pos\n",
      "    hidden_act: gelu\n",
      "  scheduler_kwargs: null\n",
      "callbacks:\n",
      "  model_checkpoint:\n",
      "    _target_: lightning.pytorch.callbacks.ModelCheckpoint\n",
      "    dirpath: ${paths.output_dir}\n",
      "    filename: '{epoch}-{step}-{val_loss:.4f}'\n",
      "    monitor: val_loss\n",
      "    save_top_k: 3\n",
      "    mode: min\n",
      "    verbose: true\n",
      "    save_last: true\n",
      "  lr_monitor:\n",
      "    _target_: lightning.pytorch.callbacks.LearningRateMonitor\n",
      "    logging_interval: step\n",
      "  progress_bar:\n",
      "    _target_: lightning.pytorch.callbacks.TQDMProgressBar\n",
      "    refresh_rate: 100\n",
      "  early_stopping:\n",
      "    monitor: val_loss\n",
      "  molecule_validation:\n",
      "    _target_: lobster.callbacks.MoleculeValidationCallback\n",
      "    num_validation_generations: ${model.num_validation_generations}\n",
      "trainer:\n",
      "  _target_: lightning.pytorch.Trainer\n",
      "  accelerator: gpu\n",
      "  devices: 1\n",
      "  num_nodes: 1\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 0.0\n",
      "  gradient_clip_algorithm: norm\n",
      "  precision: 32\n",
      "  max_steps: 100000000\n",
      "  val_check_interval: 0.25\n",
      "  limit_val_batches: 10\n",
      "  num_sanity_val_steps: 2\n",
      "  max_time: null\n",
      "setup:\n",
      "  torch:\n",
      "    _target_: torch.set_float32_matmul_precision\n",
      "    precision: medium\n",
      "  seed:\n",
      "    _target_: lightning.pytorch.seed_everything\n",
      "    seed: 15855310\n",
      "    workers: true\n",
      "lr_scheduler:\n",
      "  scheduler:\n",
      "    _target_: transformers.optimization.get_linear_schedule_with_warmup\n",
      "    num_warmup_steps: ${model.num_warmup_steps}\n",
      "    num_training_steps: ${model.num_training_steps}\n",
      "\n",
      "Seed set to 15855310\n",
      "INFO:try_load_dataset:Instantiating datamodule...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Setting up datamodule for stage: fit\n",
      "Loaded 5480540 pairs for split 'train' using utility 'gap'\n",
      "After utility filtering (> 0.001): 5204656 pairs\n",
      "Loaded 87450 pairs for split 'val' using utility 'gap'\n",
      "After utility filtering (> 0.001): 897 pairs\n",
      "Loaded 64375 pairs for split 'test' using utility 'gap'\n",
      "After utility filtering (> 0.001): 688 pairs\n"
     ]
    }
   ],
   "source": [
    "overrides = None\n",
    "config_path = '../src/lobster/hydra_config/train_molecule_improvement.yaml' #train_chembl.yaml'\n",
    "datamodule, cfg, transform_fn = load_datamodule_from_config(config_path, overrides=overrides)\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "datamodule.prepare_data()\n",
    "\n",
    "stage = \"fit\"\n",
    "print(f\"Setting up datamodule for stage: {stage}\")\n",
    "datamodule.setup(stage=stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52843bd-59ab-44f1-a3b9-567fde6bf849",
   "metadata": {},
   "source": [
    "# Check out the train, val, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d78f19b0-0e26-4ed9-9fa6-e42030cb7697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "train\n",
      "\n",
      "\n",
      "Length: 5204656\n",
      "\n",
      "Decoded 7: <cls> [H] C 1 ( [H] ) C ( [H] ) ( [H] ) C ( [H] ) ( [H] ) C ( [H] ) ( [H] ) C ( [H] ) ( [H] ) C 1 ( [H] ) [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n",
      "Decoded 8: <cls> [H] [C@] 1 2 [C@] 3 ( [H] ) [C@@] 1 ( [H] ) [C@@] 1 ( [H] ) [C@] 3 ( [H] ) [C@@] 2 1 [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n",
      "Decoded 1: <cls> [H] c 1 c ( [H] ) c ( [H] ) c ( [H] ) c ( [H] ) c 1 [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n",
      "\n",
      "\n",
      "val\n",
      "\n",
      "\n",
      "Length: 897\n",
      "\n",
      "Decoded 2: <cls> [H] C ( [H] ) ( [H] ) [C@] 1 2 [C@] 3 ( [H] ) [C@@] 1 ( [H] ) [C@@] 1 ( [H] ) [C@] 3 ( [H] ) [C@] 1 2 C ( [H] ) ( [H] ) [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n",
      "Decoded 4: <cls> [H] C ( [H] ) ( [H] ) C ( [H] ) ( [H] ) [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n",
      "Decoded 1: <cls> [H] c 1 c ( [H] ) c ( [H] ) c ( [H] ) c ( [H] ) c 1 [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n",
      "\n",
      "\n",
      "test\n",
      "\n",
      "\n",
      "Length: 688\n",
      "\n",
      "Decoded 6: <cls> [H] C 1 ( [H] ) C ( [H] ) ( [H] ) C ( [H] ) ( [H] ) C ( [H] ) ( [H] ) C 1 ( [H] ) [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n",
      "Decoded 5: <cls> [H] C 1 ( [H] ) C ( [H] ) ( [H] ) C ( [H] ) ( [H] ) C 1 ( [H] ) [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n",
      "Decoded 1: <cls> [H] c 1 c ( [H] ) c ( [H] ) c ( [H] ) c ( [H] ) c 1 [H] <sep> [H] C ( [H] ) ( [H] ) [H] <eos> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "len(datamodule._train_dataset)\n",
    "\n",
    "def check_out_dset(dset):\n",
    "    print(f'Length: {len(dset)}\\n')\n",
    "    tokenizer = dset.transform.tokenizer\n",
    "    num_check = 4\n",
    "    inds = np.random.choice(range(10), size=3, replace=False)\n",
    "    for ind in inds:\n",
    "        input_ids = dset[ind]['input_ids']\n",
    "        mask = dset[ind]['attention_mask']\n",
    "        to_decode = input_ids[mask != 0]\n",
    "        decoded = tokenizer.decode(to_decode.tolist())\n",
    "        print(f'Decoded {ind}: {decoded} \\n')\n",
    "\n",
    "dsets = {'train': datamodule._train_dataset, 'val': datamodule._val_dataset, 'test': datamodule._test_dataset}\n",
    "for ky, dset in dsets.items():\n",
    "    print(f'\\n\\n{ky}\\n\\n')\n",
    "    check_out_dset(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "979fbdbd-88bd-4fa7-8817-a06078347034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.random.choice(range(10), size=3, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08269a14-92d3-4934-ab79-84827fca273d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,   53,    8,   13,   10,   53,   11,   12,    8,   10,   53,   11,\n",
       "          10,   53,   11,   34,   14,   10,   53,   11, 1184,   17,   35,   21,\n",
       "          10,   53,   11,   34,   14,   10,   53,   11,   35,   21,   10,   53,\n",
       "          11,   34,   17,   13,   53,    5,   53,   16,   10,   53,   11,   53,\n",
       "           2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule._train_dataset[100]['input_ids'].tolist()[0]\n",
    "input_ids = datamodule._train_dataset[100]['input_ids']\n",
    "mask = datamodule._train_dataset[100]['attention_mask']\n",
    "input_ids[mask != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bd8d0f7-c7a7-4edb-99db-e666f01ee3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 53,\n",
       " 8,\n",
       " 13,\n",
       " 10,\n",
       " 53,\n",
       " 11,\n",
       " 12,\n",
       " 8,\n",
       " 10,\n",
       " 53,\n",
       " 11,\n",
       " 10,\n",
       " 53,\n",
       " 11,\n",
       " 34,\n",
       " 14,\n",
       " 10,\n",
       " 53,\n",
       " 11,\n",
       " 1184,\n",
       " 17,\n",
       " 35,\n",
       " 21,\n",
       " 10,\n",
       " 53,\n",
       " 11,\n",
       " 34,\n",
       " 14,\n",
       " 10,\n",
       " 53,\n",
       " 11,\n",
       " 35,\n",
       " 21,\n",
       " 10,\n",
       " 53,\n",
       " 11,\n",
       " 34,\n",
       " 17,\n",
       " 13,\n",
       " 53,\n",
       " 5,\n",
       " 53,\n",
       " 16,\n",
       " 10,\n",
       " 53,\n",
       " 11,\n",
       " 53,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule._train_dataset[100]['input_ids'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4678f4a-c217-4fd1-83ab-78c7623e8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = datamodule._train_dataset.transform.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d89089be-ec46-405a-8dce-1f1974726f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls> [H] C 1 ( [H] ) O C ( [H] ) ( [H] ) [C@@] 2 ( [H] ) [N@@H+] 3 [C@] 4 ( [H] ) [C@@] 2 ( [H] ) [C@] 4 ( [H] ) [C@@] 3 1 [H] <sep> [H] N ( [H] ) [H] <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(datamodule._train_dataset[100]['input_ids'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ee5af-5380-4d27-8f72-e8018b9a0244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c472e4ef-f98d-4cb6-8904-c5d18a2ca820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_target_': 'lobster.model.LobsterPCLM2', 'model_name': 'CLM_mini', 'lr': 0.0001, 'max_length': 1024, 'ckpt_path': None, 'num_training_steps': '${trainer.max_steps}', 'num_warmup_steps': 1000, 'tokenizer_dir': 'garbage_test', 'transform_fn': '${data.transform_fn}', 'model_kwargs': {'embedding_layer': 'linear_pos', 'hidden_act': 'gelu'}, 'scheduler_kwargs': None}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11148d6-cd14-4548-b1e2-33418facbedb",
   "metadata": {},
   "source": [
    "# Load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20db6005-a595-4a0f-a061-ed22a715ffcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lawrenh6/lobster_runs/val_initialized_propen_4/2025-08-12T06-51-27.224517/epoch=0-step=20330-val_loss=0.5613.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the LobsterPMLM model\n",
    "\n",
    "ckpt_path = find_latest_ckpt_file('/data/lawrenh6/lobster_runs/val_initialized_propen_4', use_val_loss=True) # serious_chembl_large2\n",
    "print(ckpt_path)\n",
    "#ckpt_path = \"/homefs/home/lawrenh6/lobster/first_run/2025-07-14T17-49-53.473790/last.ckpt\"\n",
    "#ckpt_path = \"/data/lawrenh6/lobster_runs/ignore/2025-08-01T21-34-23.837619/last.ckpt\"\n",
    "#ckpt_path = \"/data/lawrenh6/lobster_runs/ignore/2025-08-04T18-41-43.437182/last.ckpt\"\n",
    "\n",
    "# oops, should be getting transform_fn from elsewhere??\n",
    "model = LobsterPCLM2(\"CLM_150M\", transform_fn=transform_fn).to(device) # CLM_mini # CLM_150M  # CLM_mini\n",
    "model.eval()\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "model.load_state_dict(ckpt['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "408c315a-4efd-419c-9118-726a708095b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/lawrenh6/lobster_runs/val_initialized_propen_4/2025-08-12T06-51-27.224517/epoch=0-step=20330-val_loss=0.5613.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb7cd95-d64a-4c95-a0d7-18d173578e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer._tokenizer.post_processor = None  # changes by reference, seemingly -- so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beaf6bae-3db1-4a82-8044-0a9f7f79762d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(model.tokenizer.encode(\"<cls>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "872cbe69-4244-4738-a780-b84c3a9e05ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155572848"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66b0527b-005b-44bd-b9b7-c32491b336c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of seed_seq:\n",
      "[0]\n",
      "<cls>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<cls>[H] O C 1 ( [H] ) C ( = O ) C ( [H] ) ( [H] ) C ( [H] ) ( [H] ) C 1 = O [H] O [C@] 1 2 C ( [H] ) ( [H] ) C ( = O ) [C@@] 1 ( [H] ) [N@H+] 1 C ( [H] ) ( [H] ) [C@] 1 2 [H]'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do we need to add cls token or eos token?? also does it make sense to use cls token?\n",
    "model.eval()\n",
    "seed_seq = \"<cls>\"\n",
    "print('Tokenization of seed_seq:')\n",
    "print(model.tokenizer.encode(seed_seq))\n",
    "print(model.tokenizer.decode(model.tokenizer.encode(seed_seq)))\n",
    "out = model.sample(seed_seq=seed_seq, temperature=0.95, max_length=300) #CN(C)C[C@@H](O)[C@@H](c1\") #, max_length=256) # CN(C)C[C@@H](O)[C@@H](c1\n",
    "# how to add start token to this??\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "f1cf6758-b2e5-46e4-ac4c-c29f33e424d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cls> C N ( C ) C [C@@H] ( O ) [C@@H] ( c 1 c c c c c 1 ) c 1 c c c ( Cl ) c c 1\n"
     ]
    }
   ],
   "source": [
    "print(model.tokenizer.decode(model.tokenizer.encode(f\"<cls>{test_mol}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "e79d8b47-bc58-48f5-90e4-5ad8c02bcdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  8, 16, 10,  8, 11,  8, 20, 10, 12, 11, 20, 10,  9, 13,  9,  9,  9,\n",
       "          9,  9, 13, 11,  9, 13,  9,  9,  9, 10, 26, 11,  9,  9, 13,  2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.generate(torch.tensor(model.tokenizer.encode(f\"<cls>{test_mol}\")).reshape(1,-1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a6b49af-6bdb-49c0-bf09-98c86c29510c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CN(C)C[C@@H](O)[C@@H](c1ccccc1)c1ccc(Cl)cc1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fd4b0fd-e76c-4a51-b9d6-5439d79818e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  8, 16, 10,  8, 11,  8, 20, 10, 12, 11, 20, 10,  9, 13,  9,  9,  9,\n",
      "          9,  9, 13, 11,  9, 13,  9,  9,  9, 10, 26, 11,  9,  9, 13]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0,  8, 16, 10,  8, 11,  8, 20, 10, 12, 11, 20, 10,  9, 13,  9,  9,  9,\n",
      "          9,  9, 13, 11,  9, 13,  9,  9,  9, 10, 26, 11,  9,  9, 13, 53,  5, 53,\n",
      "          8, 10, 15, 12, 11, 16, 10, 53, 11,  8, 10, 53, 11, 10, 53, 11,  8]],\n",
      "       device='cuda:0')\n",
      "<cls> C N ( C ) C [C@@H] ( O ) [C@@H] ( c 1 c c c c c 1 ) c 1 c c c ( Cl ) c c 1 [H] <sep> [H] C ( = O ) N ( [H] ) C ( [H] ) ( [H] ) C\n",
      "[34]\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(model.tokenizer.encode(f\"<cls>{test_mol}\")).reshape(1,-1).to(device)\n",
    "print(input_ids)\n",
    "generated = model.model.generate(input_ids)\n",
    "print(generated)\n",
    "print(model.tokenizer.decode([int(g) for g in generated.reshape(-1)]))\n",
    "\n",
    "temp = model.tokenizer.decode([int(g) for g in generated.reshape(-1)])\n",
    "temp = temp.split(' ')\n",
    "seps = [i for (i, val) in enumerate(temp) if val == '<sep>']\n",
    "print(seps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "77db9cc5-d017-4529-9d7d-c2185958f49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82a5f69-3713-4ad4-9596-4b2f8cb982b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a | b  c d e | c'.split('|')[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b5487de7-3c12-47ca-87bf-6691e1882e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[341]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m [\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m generated]\n",
      "\u001b[31mValueError\u001b[39m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "[int(g) for g in generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a93220-4a0c-498f-be56-10e75cc8c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of seed_seq:\n",
      "[0]\n",
      "<cls>\n",
      "Fraction sane so far: 0.0, 0.0 out of 1\n",
      "Final fraction sane: 0.0, 0.0 out of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:36:06] Explicit valence for atom # 9 H, 2, is greater than permitted\n"
     ]
    }
   ],
   "source": [
    "# looped test for non-paired dataset\n",
    "num_samples = 1 #50 #300 #300 #500\n",
    "num_sane = 0.0\n",
    "seed_seq = \"<cls>\" #<cls>CN(C)C[C@@H](O)\" # no cls, just to see\n",
    "print('Tokenization of seed_seq:')\n",
    "print(model.tokenizer.encode(seed_seq))\n",
    "print(model.tokenizer.decode(model.tokenizer.encode(seed_seq)))\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "all_smiles = []\n",
    "is_sane = []\n",
    "for i in range(num_samples):\n",
    "    out = model.sample(seed_seq=seed_seq, temperature=0.95, max_length=300)\n",
    "    temp = out[0].split(\" \")\n",
    "    temp[0] = temp[0][5:]\n",
    "    smiles = ''.join(temp)\n",
    "    all_smiles.append(smiles)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    try:\n",
    "        res = is_molecule_sane(mol)\n",
    "        is_sane.append(res)\n",
    "        if res:\n",
    "            num_sane += 1\n",
    "    except Exception as e:\n",
    "        1+1; #print(f'not sane due to {e}')\n",
    "    if i % 20 == 0:\n",
    "        print(f'Fraction sane so far: {num_sane / (i+1)}, {num_sane} out of {(i+1)}')\n",
    "print(f'Final fraction sane: {num_sane / num_samples}, {num_sane} out of {num_samples}')\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce42d80-e4ec-43d2-9ec7-ca55a1c34b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about conditioning with seed seq?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4ab63e-d892-4e4b-a5fe-d8d13fd7a425",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenizer\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d95b9-730e-4cef-ab0f-ee517483837f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e91bcc-2e0b-43b8-b945-681a0dfbf86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db7b527e-0b7d-436a-80ab-e0f28492b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of seed_seq:\n",
      "[0]\n",
      "<cls>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=300) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no separator found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m num_sane1, num_sane2 = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_seq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     temp = out[\u001b[32m0\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     temp[\u001b[32m0\u001b[39m] = temp[\u001b[32m0\u001b[39m][\u001b[32m5\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lobster/src/lobster/model/_clm2.py:235\u001b[39m, in \u001b[36mLobsterPCLM2.sample\u001b[39m\u001b[34m(self, seed_seq, max_length, do_sample, temperature, top_k, repetition_penalty, num_return_sequences)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    221\u001b[39m     seed_seq: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     num_return_sequences: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m,\n\u001b[32m    228\u001b[39m ):\n\u001b[32m    229\u001b[39m     generator = pipeline(\n\u001b[32m    230\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m         model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    232\u001b[39m         tokenizer=\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m    233\u001b[39m         device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    234\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     outseqs = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# \"this is an awkward expression to step into\" -sam\u001b[39;00m\n\u001b[32m    248\u001b[39m     outseqs = [samp[\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m].replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m samp \u001b[38;5;129;01min\u001b[39;00m outseqs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:316\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    315\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1464\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1457\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1458\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1461\u001b[39m         )\n\u001b[32m   1462\u001b[39m     )\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1471\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1470\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1371\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1369\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1370\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:414\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    412\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    417\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2625\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2617\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2618\u001b[39m         input_ids=input_ids,\n\u001b[32m   2619\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2620\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2621\u001b[39m         **model_kwargs,\n\u001b[32m   2622\u001b[39m     )\n\u001b[32m   2624\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2636\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2637\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2638\u001b[39m         input_ids=input_ids,\n\u001b[32m   2639\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2640\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2641\u001b[39m         **model_kwargs,\n\u001b[32m   2642\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3609\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3607\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3609\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3611\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3612\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3613\u001b[39m     outputs,\n\u001b[32m   3614\u001b[39m     model_kwargs,\n\u001b[32m   3615\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3616\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:553\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m output_hidden_states = (\n\u001b[32m    549\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    550\u001b[39m )\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:441\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    439\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:306\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m residual = hidden_states\n\u001b[32m    305\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    309\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:151\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/envlob/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1927\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1922\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1926\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1928\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1929\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# looped test for paired dataset\n",
    "# looped test\n",
    "num_samples = 50 #300 #300 #500\n",
    "num_sane = 0.0\n",
    "seed_seq = \"<cls>\" #<cls>CN(C)C[C@@H](O)\" # no cls, just to see\n",
    "print('Tokenization of seed_seq:')\n",
    "print(model.tokenizer.encode(seed_seq))\n",
    "print(model.tokenizer.decode(model.tokenizer.encode(seed_seq)))\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "all_smiles1, all_smiles2 = [], []\n",
    "is_sane1 = []\n",
    "is_sane2 = []\n",
    "num_sane1, num_sane2 = 0, 0\n",
    "for i in range(num_samples):\n",
    "    out = model.sample(seed_seq=seed_seq, temperature=0.95, max_length=300)\n",
    "    temp = out[0].split(\" \")\n",
    "    temp[0] = temp[0][5:]\n",
    "    seps = [i for (i, val) in enumerate(temp) if val == '<sep>']\n",
    "    if len(seps) == 0:\n",
    "        print('no separator found')\n",
    "        continue\n",
    "    else:\n",
    "        print('separator found')\n",
    "    smiles1 = ''.join(temp[:seps[0]])\n",
    "    smiles2 = ''.join(temp[seps[0]:])\n",
    "    print('smiles1', smiles1, 'smiles2', smiles2)\n",
    "    all_smiles1.append(smiles1)\n",
    "    all_smiles2.append(smiles2)\n",
    "    mol1 = Chem.MolFromSmiles(smiles1)\n",
    "    mol2 = Chem.MolFromSmiles(smiles2)\n",
    "    try:\n",
    "        res = is_molecule_sane(mol1)\n",
    "        is_sane1.append(res)\n",
    "        if res:\n",
    "            num_sane1 += 1\n",
    "    except Exception as e:\n",
    "        1+1; #print(f'not sane due to {e}')\n",
    "    try:\n",
    "        res = is_molecule_sane(mol2)\n",
    "        is_sane2.append(res)\n",
    "        if res:\n",
    "            num_sane2 += 1\n",
    "    except Exception as e:\n",
    "        1+1; #print(f'not sane due to {e}')\n",
    "    if i % 20 == 0:\n",
    "        print(f'Fraction sane so far: {num_sane / (i+1)}, {num_sane} out of {(i+1)}')\n",
    "print(f'Final fraction sane: {num_sane / num_samples}, {num_sane} out of {num_samples}')\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36289f99-3b1b-4bf2-919e-9b6b5242279c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<cls>[H] O N C 1 = C ( [H] ) [C-] ( [H] ) [C@@] 2 ( C ( [H] ) ( [H] ) [H] ) C ( [H] ) ( [H] ) [C@@] 1 2 [H] [H] O N [C@@H] 1 [C@@] ( [H] ) ( C ( [H] ) ( [H] ) [H] ) [C@@] 2 ( [H] ) C ( [H] ) ( [H] ) [C@@] 1 2 C ( [H] ) ( [H] ) [H]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "933d0d55-c812-41ed-bdfd-679fb6827a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "seps = [i for (i, val) in enumerate(temp) if val == '<sep>']\n",
    "print(seps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cabd5ff-a430-4b25-8949-9385c93a9e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322da8c7-ee5b-48b1-8acd-0c02740260af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "ada31deb-b9d1-465a-81f0-d1f5e62f3339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final fraction sane: 0.84, 42.0 out of 50\n",
      "Elapsed time: 0.67 min, or 0.7996 sec per sample\n"
     ]
    }
   ],
   "source": [
    "print(f'Final fraction sane: {num_sane / num_samples}, {num_sane} out of {num_samples}')\n",
    "print(f'Elapsed time: {((end - start)/ 60.0):.2f} min, or {((end - start)/ (num_samples)):.4f} sec per sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "7607f944-19ed-4705-aab1-c168e7685ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCOC(=O)Nc1sc2c(c1C#N)CCC(C)C2',\n",
       " 'CN1C(=O)CS/C=C/COc2cc(N3CCOCC3)c(N3CCOCC3)cc21',\n",
       " 'O=C(Nc1nc2ccccc2c(=O)s1)Nc1ccccc1F',\n",
       " 'COc1ccc(CCNc2ccnc3oc4ccccc4c(=O)c23)cc1OC',\n",
       " 'COc1cc2ncnc(NCc3ccc(C)o3)c2cc1OC',\n",
       " 'COc1ccc(NC(=O)[C@@H]2CCCCNC(=O)CCCCCCC(=O)N[C@H](C(=O)N[C@@H](Cc3ccccc3)C(=O)NC(Cc3ccccc3)C(N)=O)C(C)C)cc1',\n",
       " 'CC(F)(F)c1cn[nH]c1C1CCN(c2cc(N)ncn2)CC1',\n",
       " 'CC(=O)NC1(c2cccc(C(F)(F)F)c2)CCN(c2ncnc3c2cc(C)n3C)CC1',\n",
       " 'CCCCN(CCCC)c1nccc2[nH]c3ccccc3c12',\n",
       " 'O=C(CCN1CCCCC1)Nc1nncs1',\n",
       " 'CN1[C@H](C(=O)NCc2ccc(F)c(F)c2)CCS1(=O)=O',\n",
       " 'CN(Cc1cnc2nc(N)nc(N)c2n1)c1cccCCC(=O)NC(CCC(=O)N[C@@H](CCC(=O)N[C@@H](CCC(=O)N[C@@H](CCC(=O)N[C@@H](Cc2ccccc2F)C(=O)OC(C)(C)C)C(=O)O)C(=O)O)C(=O)O',\n",
       " 'NC(=O)NC(=O)CCC1C2CC3OC(=O)C1C3C2',\n",
       " 'O=S(=O)(c1cccc(Oc2ncc(Cl)cc2NS(=O)(=O)C(F)(F)F)c1)N1CCc2ccccc2C1',\n",
       " 'COCC(=N)Nc1nnc(SCC(=O)Nc2c(C)cccc2C)s1.Cl',\n",
       " 'CN[C@@H](C)C(=O)N[C@H](C(=O)N[C@@H](CC(C)C)C(=O)Nc1nccc2ccccc12)C(C)C.O=CO',\n",
       " 'O=C1C=C(/C=C/c2ccccc2)O[C@H](COCc2ccccc2)C1',\n",
       " 'C[S+]([O-])c1nc(-c2ccccc2)c(Cc2ccccc2)(=O)[nH]1',\n",
       " 'O=C(/N=C1\\\\SC2CS(=O)(=O)CC2N1c1cccc(Cl)c1)C1CCC1',\n",
       " 'COc1ccc(C2=NO[C@@]3(CC[C@@H](C(C)(C)C)CC3)S2)cc1',\n",
       " 'CC(=O)n1c(-c2ccc(F)cc2)c(CCN2CCOCC2)c2ccccc21',\n",
       " 'COc1cc(C#N)cnc1C(=O)Nc1cncc(C(=O)c2cn3ccc4ccccc4c3n2)c1',\n",
       " 'C=CC(=O)Nc1cc2c(Nc3ccc(Br)c(Cl)c3F)ncnc2cc1OCCOC',\n",
       " 'O=C(Sc1nnc(-c2cccc(Cl)c2)[nH]1)C1CCCC1',\n",
       " 'Cc1noc2cc(Oc3ccncc3C(=O)N3CCN(C4CC4)c4ccccc43)ccc12',\n",
       " 'N=C(N)c1cccc(Nc2cnccc2C(=O)Nc2ccc(-c3ccccc3S(N)(=O)=O)cc2)c1.O=C(O)C(F)(F)F',\n",
       " 'CC[C@H](C)[C@H](NC(=O)[C@@H](NC(=O)[C@@H]1CCC[C@H]1C)C1(C)OCCO1)C(=O)N(C)CC(=O)N(C)Cc1ccccn1',\n",
       " 'CC(C)NCc1cnc(C(c2ccccc2)c2ccccc2)o1',\n",
       " 'CC(C)(COc1cnn(-c2cccc(Cl)c2)c(=O)c1Oc1ccc(C(=O)c2ccccc2)cc1)C(=O)O',\n",
       " 'O=S(=O)(Nc1n[nH]c2ccc(C3=CCNCC3)cc12)c1ccccc1',\n",
       " 'O=C(CCCn1cc(CN2CCN(Cc3ccccc3)CC2)c2ccccc21)c1ccc(F)cc1',\n",
       " 'CCOc1cc2c3cc(OC)c(OC)cc3c(CC)[n+](C)c2c2cc(OC)c(OC)cc12.[O-][Cl+3]([O-])([O-])[O-]',\n",
       " 'CCN(CC)S(=O)(=O)c1ccc(NC(=O)Cc2csc(-c3ccccc3)n2)cc1',\n",
       " 'CC[N+](C)(CC)CC#Cc1ccc2ncccc2c1.O=C([O-])c1cccnc1',\n",
       " 'O=C1CC(c2c[nH]c3ccccc23)Sc2ccccc2N1Cc1cccc([N+](=O)[O-])c1',\n",
       " 'CN(Cc1ccco1)/C(=N\\\\O)c1cccnc1Oc1cccc2c1C(C)(C)CN2',\n",
       " 'CC(=O)N1CCCc2nc(-n3c(C)cc4c3CCC(C(=O)O)C4)ccc21',\n",
       " 'C[C@]12CC[C@H](CCCc3ccccc3)C(=O)N1[C@H](C(=O)NC(CCCN=C(N)N)C(=O)c1nccs1)CS2',\n",
       " 'O=C1NCc2c(ccc3c2OCCO3)N1c1ccc2nc[nH]c2c1',\n",
       " 'COc1cc(N2CCc3ncnc(O[C@H]4CCN(C(=O)c5cnoc5)C4)c3C2)cnc1OC',\n",
       " 'CC(=O)c1ccc(NC(=NS(=O)(=O)c2ccc(F)cc2)c2ccccc2)cc1',\n",
       " 'COc1ccc(CN2C[C@@]3(COO[C@H](C)C[C@H](C)N3C)OCC2=O)cc1',\n",
       " 'O=c1[nH]n(-c2nc3ccccc3s2)c(=O)c2ccccc12',\n",
       " 'Cc1cc(NC(=O)CN2C(=O)C(=O)N(Cc3cccc(C)c3)C2=O)no1',\n",
       " 'C=C(C)[C@@H]1CC[C@]2(/C=N/O)CC[C@]3(C)[C@H](CC[C@@H]4[C@@]5(C)CC[C@@H](O)C(C)(C)[C@@H]5CC[C@]43C)[C@@H]12',\n",
       " 'COc1cc2nc(/N=C\\\\c3ccc([N+](=O)[O-])o3)nc(N)c2cc1OC',\n",
       " 'CC(=O)CC(C)(C)NC(=O)c1ccc(C2CCCO2)s1',\n",
       " 'COc1cc(C(=O)NCC2CCN(CC3CCCCC3)CC2)ccc1-n1ncc2cc(Nc3ccccc3)ccc2c1=O',\n",
       " 'Nc1nc(N)c2nc(COC(=O)CNS(=O)(=O)c3cccc4cccnc34)cnc2n1',\n",
       " 'CC(C)C[C@H](N[C@@H](c1ccc(-c2ccc(S(C)(=O)=O)cc2)cc1)C(F)(F)F)C(=O)NC(C#N)CCCCN']"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_smiles"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5d4ee67-4e89-4f88-80e1-b79aa3184478",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "d80520c2-042d-4d14-bc7a-95e8e72ae488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls>'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "763a7ba3-7832-472c-bfa9-c6e38f3317f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sep>'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6c3ae-b3ae-4c49-91ac-507d4ae3a1e7",
   "metadata": {},
   "source": [
    "# Check tokenizers: look at vocabulary, compare tokenizers, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "801eb239-0bf2-437a-864f-2853a880d232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls> [H] C 1 ( [H] ) [N@H+] 2 [C@@] 3 ( [H] ) [C@@] 1 4 O [C@@] 1 ( [H] ) [C@@] ( [H] ) ( [C@] 2 4 [H] ) [C@] 1 3 [H] <sep> [H] C 1 ( [H] ) [C@@] 2 ( [H] ) [C@@] 3 ( [H] ) [C@@] 1 4 O [C@] 1 ( [H] ) [C@@] 3 ( [H] ) [C@] 1 ( [H] ) [C@] 2 4 [H] <eos>'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = datamodule._train_dataset[-1]['attention_mask'] != 0\n",
    "' '.join([tokenizer.decode(i) for i in datamodule._train_dataset[-1]['input_ids'][mask]])\n",
    "# note: the mask mlay be very small piece of data, which indicates should change padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fb208e33-66a1-49af-bbaf-1fdea1d11c9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer.convert_ids_to_tokens(out['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8255b-05d1-433f-8c27-a4775a162c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model._transform_fn.tokenizer\n",
    "tokenizer2 = datamodule._train_dataset.transform.tokenizer\n",
    "deep_compare(tokenizer2, tokenizer, fail_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f398c92-ddfb-4bb8-9af8-0574b00318c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.tokenizer._tokenizer.post_processor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d076b85-82ef-4cde-aeea-50c759e63119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f8aec448-190d-4006-94e4-e1c0f55b4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt['hyper_parameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6a6ec-a819-4bd4-b795-995562300637",
   "metadata": {},
   "source": [
    "# look at next probs for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb897c40-f31e-4aa9-bd77-c957232dbe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_nll, test_logits = model.get_nll_and_logits(test_mol)\n",
    "next_probs = test_logits.softmax(-1)[:, -1]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(torch.arange(1226), next_probs.cpu().squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5661c3a-2c74-49ab-98a5-7dfc343f688b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71939b42-7522-4c9e-a761-b43425f1f7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28160ad-bb39-4002-a00d-c870d29157bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69662260-9ff7-468f-9d2e-d8f2fe672483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eb4c9e-89ad-4edb-9777-4478b2090c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envlob",
   "language": "python",
   "name": "envlob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
