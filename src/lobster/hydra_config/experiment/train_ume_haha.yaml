# @package _global_

# lobster_train experiment=train_ume logger.entity=zadorozk logger.project=lobster logger.group=ume-dev ++trainer.accelerator=gpu ++trainer.devices=auto 

# @package _global_

defaults:
  - override /model: modern_bert.yaml
  - override /data: ume
  - override /callbacks: [base, throughput]

data:
  _target_: lobster.data.UmeHahaLightningDataModule
  datasets: ["M320M"]
  root: ${paths.root_dir}/data
  batch_size: 16
  tokenizer_max_length: ${model.max_length}

paths:
  root_dir: dev
  
trainer:
  max_steps: 50_000
  val_check_interval: 1_000
  precision: 16-mixed
  gradient_clip_val: 0.5
  accumulate_grad_batches: 8
  max_time: "00:24:00:00"


model:
  max_length: 512
  num_warmup_steps: 1_000
  tokenizer_dir: nucleotide_tokenizer
  num_training_steps: ${trainer.max_steps}
  lr: 4e-4
  mask_percentage: 0.25
  hidden_size: 1008
  num_hidden_layers: 36
  num_attention_heads: 12
  intermediate_size: 2304
  beta1: 0.9
  beta2: 0.98
  eps: 1e-12
  vocab_size:  627
  pad_token_id: 1
  cls_token_id: 0
  mask_token_id: 4
  sep_token_id: 5
  eos_token_id: 2
  unpad_embeddings: true

callbacks:
  moleculeace_linear_probe:
    max_length: ${model.max_length}
    run_every_n_epochs: 1


logger:
  name: ume_haha_${now:%Y-%m-%d_%H-%M-%S}