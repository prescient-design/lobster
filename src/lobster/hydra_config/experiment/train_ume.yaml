# @package _global_

defaults:
  - override /data: ume
  - override /callbacks: [base]
  - override /model: neobert

compile: false

data:
  _target_: lobster.data.UMELightningDataModule
  datasets:
    - AMPLIFY
    - PeptideAtlas
    - M320M
    - Calm
    - ZINC
  max_length: ${model.max_length}
  seed: 0
  batch_size: 32
  num_workers: 8


model:
  lr: 1e-5
  scheduler: constant_with_warmup
  scheduler_kwargs:
    num_warmup_steps: 20_000
  weight_decay: 0.01
  mask_probability: 0.15
  ckpt_path: null
  model_kwargs:
    max_length: ${data.max_length}
    model_size: large


logger:
  name: ${model.model_name}_${model.max_length}_${model.lr}_${paths.timestamp}
  project: lobster
  entity: ${oc.env:LOBSTER_USER}
  save_dir: ${oc.env:LOBSTER_RUNS_DIR}


paths:
  timestamp: ${now:%Y-%m-%d}T${now:%H-%M-%S} # ${now:%Y-%m-%d}T${now:%H-%M-%S.%f}
  output_dir: ${paths.root_dir}/${paths.timestamp}
  root_dir: ${oc.env:LOBSTER_RUNS_DIR}


trainer:
  num_nodes: 1
  max_steps: ${model.num_training_steps}
  max_epochs: -1
  gradient_clip_val: 0.5
  max_time: null
  val_check_interval: 500
  limit_val_batches: 10_000
  precision: 16-mixed
  accumulate_grad_batches: 16
  devices: auto


callbacks:
  model_checkpoint:
    save_top_k: -1
    every_n_train_steps: 500
  # batch_size_finder:
  #   _target_: lightning.pytorch.callbacks.BatchSizeFinder
  #   mode: power
  #   steps_per_trial: 3
  #   init_val: 64
  #   max_trials: 25
  #   batch_arg_name: batch_size
