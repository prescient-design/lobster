# @package _global_

defaults:
  - override /data: null
  - override /callbacks: [base]
  - override /model: null
  - override /lr_scheduler: null

compile: false

# rdkit_descriptors:
#   - MolWt
#   - NumHAcceptors
#   - NumHDonors
#   - TPSA
#   - SlogP_VSA6
#   - BertzCT

data:
  _target_: lobster.data.UMELightningDataModule
  datasets:
    - ZINC
  root: ${oc.env:LOBSTER_DATA_DIR}
  max_length: 1024
  seed: 0
  batch_size: 64
  pin_memory: true
  num_workers: 8
  use_shared_tokenizer: false
  # dataset_kwargs:
  #   ZINC:
  #     use_shared_tokenizer: false
  #     extra_transform_fns:
  #       rdkit:
  #         _target_: lobster.transforms.SmilesToRDKitDescriptorsTransform
  #         normalize: true
  #         invert: true
  #         descriptor_list: ${rdkit_descriptors}

model:
  _target_: lobster.model.ume2.UMESequenceEncoderLightningModule 
  # auxiliary_tasks:
  #   - _target_: lobster.model.ume2.AuxiliaryTask
  #     name: rdkit
  #     task_type: regression
  #     output_dim: 6
  #     hidden_size: null
  #     loss_weight: 0.05
  #     pooling: cls
  #     num_layers: 2
  #     dropout: 0.1
  lr: 1e-5
  scheduler: cosine
  scheduler_kwargs:
    num_warmup_steps: 1_000
    num_training_steps: 100_000
  weight_decay: 0.01
  mask_token_id: 4
  pad_token_id: 1
  special_token_ids: [0, 1, 2, 3, 4, 5]
  mask_probability: 0.3
  ckpt_path: #s3://prescient-lobster/ume/runs/${paths.timestamp}/last.ckpt
  encoder_kwargs:
    max_length: ${data.max_length}
    cache_dir: /data2/ume/checkpoints
    model_size: small
    vocab_size: 1226

logger:
  name: UME2-${model.encoder_kwargs.model_size}_${data.datasets}_smiles_no_aux_${paths.timestamp}
  project: lobster
  entity: ${oc.env:LOBSTER_USER}
  save_dir: ${oc.env:LOBSTER_RUNS_DIR}
  # id: fvxv9lff
  # resume: must
  # allow_val_change: true

paths:
  timestamp: ${now:%Y-%m-%d}T${now:%H-%M-%S} 
  output_dir: ${paths.root_dir}/${paths.timestamp}
  root_dir: ${oc.env:LOBSTER_RUNS_DIR}

trainer:
  num_nodes: 1
  max_epochs: -1
  gradient_clip_val: 0.5
  max_time: null
  max_steps: ${model.scheduler_kwargs.num_training_steps}
  val_check_interval: 500
  limit_val_batches: 50_000
  precision: 16-mixed
  accumulate_grad_batches: 4
  devices: auto

callbacks:
  model_checkpoint:
    save_top_k: -1
    every_n_train_steps: 500
  # batch_size_finder:
  #   _target_: lightning.pytorch.callbacks.BatchSizeFinder
  #   mode: power
  #   steps_per_trial: 3
  #   init_val: 64
  #   max_trials: 25
  #   batch_arg_name: batch_size
