_target_: lobster.model.LobsterPCLM2

# Model parameters
model_name: "CLM_mini"  # Start with smaller model for faster training
lr: 1e-4  # Lower learning rate for finetuning
max_length: 512
ckpt_path: null  # Path to pretrained model if available

# Training parameters
num_training_steps: ${trainer.max_steps}
num_warmup_steps: 1000

# Tokenizer configuration - use SMILES tokenizer instead of protein tokenizer
tokenizer_dir: "garbage_test" #"smiles_tokenizer"  # This will use the SMILES tokenizer
transform_fn: ${data.transform_fn}

# Model-specific configuration parameters
model_kwargs:
  embedding_layer: linear_pos 
  hidden_act: gelu

# Scheduler-specific configuration parameters
scheduler_kwargs:
  # Any specific scheduler parameters would go here 