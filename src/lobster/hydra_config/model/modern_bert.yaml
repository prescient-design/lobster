_target_: lobster.model.modern_bert.FlexBERT

lr: 1e-3
num_training_steps: ${trainer.max_steps}
num_warmup_steps: 10_000
max_length: 512
tokenizer_dir: pmlm_tokenizer
embedding_layer: linear_pos
hidden_size: 252
num_hidden_layers: 12
num_attention_heads: 12
intermediate_size: 1152
hidden_act: gelu
mask_percentage: 0.25
ckpt_path: null
