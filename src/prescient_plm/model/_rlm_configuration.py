RLM_CONFIG_ARGS = {
    "RLM_micro": {
        "num_labels": 1,
        "problem_type": "regression",
        "num_hidden_layers": 3,
        "num_attention_heads": 6,
        "intermediate_size": 64,
        "hidden_size": 72,
        "attention_probs_dropout_prob": 0.0,
        "mask_token_id": 32,
        "pad_token_id": 1,
        "token_dropout": False,
        "position_embedding_type": "rotary",
        "vocab_size": 33,
        "layer_norm_eps": 1e-12,
    },
    "RLM_mini": {
        "num_labels": 1,
        "problem_type": "regression",
        "num_hidden_layers": 4,
        "num_attention_heads": 8,
        "intermediate_size": 1280,
        "hidden_size": 320,
        "attention_probs_dropout_prob": 0.0,
        "mask_token_id": 32,
        "pad_token_id": 1,
        "token_dropout": False,
        "position_embedding_type": "rotary",
        "vocab_size": 33,
        "layer_norm_eps": 1e-12,
    },
    "RLM_small": {
        "num_labels": 1,
        "problem_type": "regression",
        "num_hidden_layers": 32,
        "num_attention_heads": 12,
        "intermediate_size": 1280,
        "hidden_size": 480,
        "attention_probs_dropout_prob": 0.0,
        "mask_token_id": 32,
        "pad_token_id": 1,
        "token_dropout": False,
        "position_embedding_type": "rotary",
        "vocab_size": 33,
        "layer_norm_eps": 1e-12,
    },
    "RLM_med": {
        "num_labels": 1,
        "problem_type": "regression",
        "num_hidden_layers": 30,
        "num_attention_heads": 20,
        "intermediate_size": 2560,
        "hidden_size": 640,
        "attention_probs_dropout_prob": 0.0,
        "mask_token_id": 32,
        "pad_token_id": 1,
        "token_dropout": True,
        "position_embedding_type": "rotary",
        "vocab_size": 33,
        "layer_norm_eps": 1e-05,
    },
    "RLM_cram_med": {
        "num_hidden_layers": 30,
        "num_attention_heads": 20,
        "intermediate_size": 2560,
        "hidden_size": 640,
        "token_dropout": True,
        "query_bias": False,
        "key_bias": False,
        "value_bias": False,
        "intermediate_bias": False,
    },
    "RLM_cram_small": {
        "num_hidden_layers": 12,
        "num_attention_heads": 12,
        "intermediate_size": 2048,
        "hidden_size": 768,
        "query_bias": False,
        "key_bias": False,
        "value_bias": False,
        "intermediate_bias": False,
    },
}
