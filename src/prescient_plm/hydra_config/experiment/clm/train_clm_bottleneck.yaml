# @package _global_

defaults:
  - override /model: clm.yaml
  - override /data/transform_fn: pmlm_tokenizer_transform.yaml

paths:
  root_dir: /data/bucket/freyn6/models/pclm/debug

data:
  path_to_fasta: [
    '/data/bucket/freyn6/data/uniref50.fasta',
  ]
  batch_size: 128
  num_workers: 8
  pin_memory: False
  lengths: [0.9,0.05,0.05]
  max_length: 512
  tokenizer_dir: ${model.tokenizer_dir}
  mlm: False

  transform_fn:
    tokenizer_dir: ${model.tokenizer_dir}
    max_length: ${data.max_length}
    mlm: False


trainer:
  max_steps: 1_000_000_000
  # max_steps: 16_000
  accumulate_grad_batches: 8
  val_check_interval: 50
  precision: 16
  num_sanity_val_steps: 0
  gradient_clip_val: 0.5

model:
  model_name: CLM_bottleneck
  tokenizer_dir: pmlm_tokenizer_32
  max_length: ${data.max_length}
  num_warmup_steps: 1_000
  num_training_steps: ${trainer.max_steps}
  lr: 4e-4
  attention_bias: False
