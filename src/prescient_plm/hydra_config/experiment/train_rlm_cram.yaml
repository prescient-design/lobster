# @package _global_

defaults:
  - override /model: rlm.yaml
  - override /data/transform_fn: pmlm_tokenizer_transform.yaml


paths:
  root_dir: /data/bucket/prescient_plm/models-rlm

data:
  path_to_fasta: [
    '/data/bucket/freyn6/data/uniref50.fasta'
  ]
  batch_size: 8
  num_workers: 8
  pin_memory: False
  is_relative_model: True
  lengths: [0.9,0.05,0.05]
  max_length: 512

trainer:
  max_steps: 100_000_000
  num_sanity_val_steps: 0
  val_check_interval: 1000  # every n batches
  precision: 16
  gradient_clip_val: 0.5


callbacks:
  model_checkpoint:
    save_top_k: 10
  early_stopping:
    patience: 10000000000000000
model:
  model_name: RLM_cram_small
  num_training_steps: ${trainer.max_steps}
  num_warmup_steps: 10_000
  lr: 1e-3
  mask_percentage: 0.25
  beta1: 0.9
  beta2: 0.98